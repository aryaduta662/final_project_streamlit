{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Filtering Dataset**"
      ],
      "metadata": {
        "id": "Xpu5b9dT3PL7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNqsJLKf2o0_",
        "outputId": "ee0bc6a1-0a7a-40b4-eed3-765f91017787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=5f53dd46efbfb28b82266fca695a0ff22c0e54a907df20dd267cb4e3c83d9ab9\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpgJ4goL3Kmb",
        "outputId": "824826c7-9a19-4e5f-9301-88567f055d6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/DataMining/Dataset_Kinerja Pemerintah.csv')\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyHVw1QZ3Vml",
        "outputId": "ee93b81a-0e27-46f2-bce9-c6e58486aceb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['created_at', 'full_text', 'tweet_url', 'user_id_str', 'username']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from langdetect import detect, DetectorFactory\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# untuk hasil deteksi bahasa yang konsisten\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# --- Konfigurasi ---/content/\n",
        "input_file = '/content/drive/MyDrive/DataMining/Dataset_Kinerja Pemerintah.csv'\n",
        "output_file = '/content/drive/MyDrive/DataMining/Dataset_Kinerja Pemerintah_filtered.csv'\n",
        "log_file = '//content/drive/MyDrive/DataMining/Datasetfiltering_log_batch1.txt'\n",
        "\n",
        "# --- Load data ---\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# asumsi kolom teks namanya 'tweet'\n",
        "if 'full_text' not in df.columns:\n",
        "    raise ValueError(\"Pastikan kolom teks bernama 'full_text' ada di CSV\")\n",
        "\n",
        "total_awal = len(df)\n",
        "\n",
        "# --- 1. Hapus duplikat ---\n",
        "df = df.drop_duplicates(subset='full_text')\n",
        "setelah_duplikat = len(df)\n",
        "\n",
        "# --- 2. Hapus tweet terlalu pendek (<3 kata) ---\n",
        "df = df[df['full_text'].apply(lambda x: len(str(x).split()) >= 3)]\n",
        "setelah_pendek = len(df)\n",
        "\n",
        "# --- 3. Hapus tweet non-Bahasa Indonesia ---\n",
        "def is_indonesian(text):\n",
        "    try:\n",
        "        return detect(text) == 'id'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "df = df[df['full_text'].apply(is_indonesian)]\n",
        "setelah_bahasa = len(df)\n",
        "\n",
        "# --- Simpan hasil filter ---\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "# --- Catat log ---\n",
        "with open(log_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"Filtering Data Batch 1 - {datetime.now()}\\n\")\n",
        "    f.write(f\"Total awal: {total_awal}\\n\")\n",
        "    f.write(f\"Setelah hapus duplikat: {setelah_duplikat}\\n\")\n",
        "    f.write(f\"Setelah hapus tweet pendek: {setelah_pendek}\\n\")\n",
        "    f.write(f\"Setelah hapus non-Bahasa Indonesia: {setelah_bahasa}\\n\")\n",
        "    f.write(f\"Total akhir: {len(df)}\\n\")\n",
        "    f.write(\"File hasil: \" + output_file + \"\\n\")\n",
        "\n",
        "print(\"Filtering selesai.\")\n",
        "print(f\"Data awal: {total_awal}, Data akhir: {len(df)}\")\n"
      ],
      "metadata": {
        "id": "NkT2wzZh3avg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# baca file hasil filter lengkap\n",
        "df = pd.read_csv('/content/drive/MyDrive/DataMining/Dataset_Kinerja Pemerintah_filtered.csv')\n",
        "\n",
        "# pilih kolom yang dibutuhkan untuk labeling\n",
        "df_label = df[['full_text', 'username', 'created_at']].copy()\n",
        "\n",
        "# tambah kolom label kosong\n",
        "df_label['label'] = ''\n",
        "\n",
        "# simpan file siap-labeling\n",
        "df_label.to_csv('/content/drive/MyDrive/DataMining/Dataset_labeling.csv', index=False)\n",
        "\n",
        "print(\"File siap-labeling (dengan created_at) sudah dibuat.\")\n"
      ],
      "metadata": {
        "id": "NzidKdm_3bfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final_Label"
      ],
      "metadata": {
        "id": "LaUJbY6P31wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Ganti path sesuai lokasi file kamu di Drive\n",
        "path = '/content/drive/MyDrive/DataMining/Dataset_labeling - Dataset_labeling.csv.csv'\n",
        "\n",
        "df = pd.read_csv(path)\n",
        "\n",
        "def majority_vote(row):\n",
        "    labels = [row['label1'], row['label2'], row['label3']]\n",
        "    return Counter(labels).most_common(1)[0][0]\n",
        "\n",
        "df['final_label'] = df.apply(majority_vote, axis=1)\n",
        "\n",
        "# Simpan hasil ke Drive\n",
        "df.to_csv('/content/drive/MyDrive/DataMining/Dataset_Final_labeling.csv', index=False)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "XUisvbOP3400"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baca file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DataMining/Dataset_Final_labeling.csv\")  # ganti path kalau perlu\n",
        "\n",
        "# Normalisasi kolom jadi lowercase + hapus spasi berlebih\n",
        "df['final_label'] = df['final_label'].astype(str).str.strip().str.lower()\n",
        "\n",
        "# Hitung jumlah tiap label\n",
        "label_counts = df['final_label'].value_counts()\n",
        "\n",
        "# Tampilkan hasil\n",
        "print(\"Jumlah masing-masing label:\")\n",
        "print(label_counts)\n",
        "\n",
        "# (opsional) tampilkan persentase\n",
        "print(\"\\nPersentase masing-masing label:\")\n",
        "print((label_counts / label_counts.sum() * 100).round(2))\n"
      ],
      "metadata": {
        "id": "YyuY9G1738HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "CwgB1Jcm3_SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "GNU2af9Y4Ait"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/DataMining/Dataset_Final_labeling.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "XQoSniKF4MWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.describe()\n",
        "df['final_label'].value_counts()"
      ],
      "metadata": {
        "id": "c-f7srzo4QDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=df['final_label'])\n",
        "plt.title('Distribusi Kategori (Positif, Negatif)')\n",
        "plt.xlabel('final_label')\n",
        "plt.ylabel('Jumlah Data')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7-zGpPjK4UOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Cloud Keseluruhan\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Gabungkan semua teks dari dataset final\n",
        "all_text = \" \".join(df['full_text'].astype(str))\n",
        "\n",
        "# Setting WordCloud (background putih)\n",
        "wc = WordCloud(\n",
        "    width=1000,\n",
        "    height=600,\n",
        "    background_color='white',\n",
        "    max_words=300\n",
        ").generate(all_text)\n",
        "\n",
        "# Visualisasi\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud Keseluruhan Dataset (Semua Label)\", fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y_eKkP8T4VEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pisahkan teks berdasarkan label\n",
        "text_Positif = \" \".join(df[df['final_label']==\"Positif\"]['full_text'])\n",
        "text_Negatif = \" \".join(df[df['final_label']==\"Negatif\"]['full_text'])\n",
        "\n",
        "# Setting WordCloud warna putih\n",
        "wc_settings = {\n",
        "    \"width\": 800,\n",
        "    \"height\": 400,\n",
        "    \"background_color\": \"white\",\n",
        "}\n",
        "\n",
        "wc_Positif = WordCloud(**wc_settings).generate(text_Positif)\n",
        "wc_Negatif = WordCloud(**wc_settings).generate(text_Negatif)\n",
        "\n",
        "# Plot dalam satu baris\n",
        "plt.figure(figsize=(20,6))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.imshow(wc_Positif, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud - Positif\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.imshow(wc_Negatif, interpolation='bilinear')\n",
        "plt.title(\"Word Cloud - Negatif\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1n08iHd24bCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregasi Tweet per Bulan\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pastikan kolom tanggal bernama 'created_at' (ganti jika beda)\n",
        "df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
        "\n",
        "# Hapus baris yang tanggalnya tidak valid\n",
        "df = df.dropna(subset=['created_at'])\n",
        "\n",
        "# Buat kolom \"year_month\" untuk agregasi per bulan\n",
        "df['year_month'] = df['created_at'].dt.to_period('M')\n",
        "\n",
        "# Hitung jumlah tweet per bulan\n",
        "monthly_count = df.groupby('year_month').size().reset_index(name='count')\n",
        "\n",
        "# Convert period → datetime agar bisa di-plot\n",
        "monthly_count['year_month'] = monthly_count['year_month'].dt.to_timestamp()\n",
        "\n",
        "# Plot line chart\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(monthly_count['year_month'], monthly_count['count'], linewidth=3)\n",
        "plt.title(\"Tren Jumlah Tweet tentang kinerja pemerintah\")\n",
        "plt.xlabel(\"Bulan\")\n",
        "plt.ylabel(\"Jumlah Tweet\")\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n7s7VXAO4f0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Proses"
      ],
      "metadata": {
        "id": "3hKnOgp34kN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "wTUZNO5B4le2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ganti dengan hasil copy path dari Google Drive\n",
        "file_path = '/content/drive/MyDrive/DataMining/Dataset_Final_labeling.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "uDteyVzH4r8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_col = None\n",
        "for col in df.columns:\n",
        "    if \"text\" in col.lower() or \"tweet\" in col.lower() or \"komentar\" in col.lower() or \"full\" in col.lower():\n",
        "        text_col = col\n",
        "        break\n",
        "\n",
        "if not text_col:\n",
        "    text_col = df.select_dtypes(include=[\"object\"]).columns[0]\n",
        "\n",
        "print(\"Kolom teks terpakai:\", text_col)\n",
        "\n",
        "raw_text = df[text_col].dropna().astype(str)"
      ],
      "metadata": {
        "id": "j409Q-jE4uVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "3jboyNE54wEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "stopwords = set(StopWordRemoverFactory().get_stop_words())\n",
        "\n",
        "def cleaning(text):\n",
        "    text = text.lower()  # lowercase\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # hapus URL\n",
        "    text = re.sub(r\"@\\w+\", \"\", text)  # hapus mention\n",
        "    text = re.sub(r\"#\\w+\", \"\", text)  # hapus hashtag\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", text)  # hapus punctuation\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # hapus angka\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()  # hapus spasi berlebih\n",
        "    return text\n",
        "\n",
        "df[\"clean_text\"] = raw_text.apply(cleaning)\n",
        "\n",
        "# Hapus stopwords\n",
        "def remove_stopwords(text):\n",
        "    return \" \".join([word for word in text.split() if word not in stopwords])\n",
        "\n",
        "df[\"clean_text\"] = df[\"clean_text\"].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "vsbypfMB4x_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#WorldCloud Setelah Preproses\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_cleaned = \" \".join(df[\"clean_text\"].tolist())\n",
        "\n",
        "wc = WordCloud(width=2000, height=1000, background_color=\"white\").generate(text_cleaned)\n",
        "\n",
        "plt.figure(figsize=(14,7))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.title(\"WordCloud Setelah Preprocessing\", fontsize=16)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "wc.to_file(\"wordcloud_after_preprocessing.png\")\n",
        "print(\"WordCloud after save:\", \"wordcloud_after_preprocessing.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "zlnAnp7q40v_",
        "outputId": "1cc5e03e-a2ed-48fe-c289-630f3b4b33df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'clean_text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'clean_text'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2973576527.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtext_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"clean_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'clean_text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "tokens = text_cleaned.split()\n",
        "counter = Counter(tokens)\n",
        "top20 = counter.most_common(20)\n",
        "\n",
        "words, counts = zip(*top20)\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.barh(words[::-1], counts[::-1])\n",
        "plt.title(\"Top 20 Kata Setelah Preprocessing\")\n",
        "plt.xlabel(\"Frekuensi Kata\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4clxECYF43Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"Dataset_after_preproses_fix.csv\", index=False, encoding=\"utf-8\")\n",
        "print(\"Dataset cleaned berhasil disimpan → Dataset_after_preposes.csv\")"
      ],
      "metadata": {
        "id": "QCFZq3HJ5Beh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[\"clean_text\"]\n",
        "y = df[\"final_label\"]  # pastikan kolom label sudah ada\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "muQ5OwXO5EOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "bow = CountVectorizer()\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "X_train_bow = bow.fit_transform(X_train)\n",
        "X_test_bow = bow.transform(X_test)\n",
        "\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)"
      ],
      "metadata": {
        "id": "fbCmoya05F9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "models = {\n",
        "    \"Skenario 1 - Logistic Regression (TF-IDF)\" : LogisticRegression(max_iter=200),\n",
        "    \"Skenario 2 - SVM Linear (TF-IDF)\" : LinearSVC(),\n",
        "    \"Skenario 3 - Multinomial NB (TF-IDF)\" : MultinomialNB(),\n",
        "    \"Skenario 4 - Random Forest (BoW)\" : RandomForestClassifier(),\n",
        "    \"Skenario 5 - Decision Tree (BoW)\" : DecisionTreeClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "A6Y3TAfP5HwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skenario"
      ],
      "metadata": {
        "id": "nycCGNI95OmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "lci4vDjZ5JpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ganti path ini dengan path Google Drive Anda\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DataMining/Dataset_after_preproses_fix.csv\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "US0lDV-N5Qas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['clean_text']\n",
        "y = df['final_label']\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "CtiXtOyd5SdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "Gw71Kjaw5UuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_lr = LogisticRegression(class_weight='balanced', max_iter=2000)\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "pred_lr = model_lr.predict(X_test)\n",
        "\n",
        "print(\"=== Logistic Regression ===\")\n",
        "print(classification_report(y_test, pred_lr))"
      ],
      "metadata": {
        "id": "eKH6Yqm75pZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "model_svm = LinearSVC(class_weight='balanced')\n",
        "model_svm.fit(X_train, y_train)\n",
        "\n",
        "pred_svm = model_svm.predict(X_test)\n",
        "\n",
        "print(\"=== SVM ===\")\n",
        "print(classification_report(y_test, pred_svm))\n"
      ],
      "metadata": {
        "id": "YWUvsnQp5rlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_rf = RandomForestClassifier(class_weight='balanced', n_estimators=300)\n",
        "model_rf.fit(X_train, y_train)\n",
        "\n",
        "pred_rf = model_rf.predict(X_test)\n",
        "\n",
        "print(\"=== Random Forest ===\")\n",
        "print(classification_report(y_test, pred_rf))\n"
      ],
      "metadata": {
        "id": "OZ8Ej4Wr5t_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model_nb = MultinomialNB()\n",
        "model_nb.fit(X_train, y_train)\n",
        "\n",
        "pred_nb = model_nb.predict(X_test)\n",
        "\n",
        "print(\"=== Naive Bayes ===\")\n",
        "print(classification_report(y_test, pred_nb))\n"
      ],
      "metadata": {
        "id": "cXfABpwr5vxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model_knn = KNeighborsClassifier(n_neighbors=7)\n",
        "model_knn.fit(X_train, y_train)\n",
        "\n",
        "pred_knn = model_knn.predict(X_test)\n",
        "\n",
        "print(\"=== KNN ===\")\n",
        "print(classification_report(y_test, pred_knn))\n"
      ],
      "metadata": {
        "id": "Y67ImMYK5xfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "2M63Wfsp50Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"clean_text\"]\n",
        "y = df[\"final_label\"]\n"
      ],
      "metadata": {
        "id": "-TXFgZQn51WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_tfidf = tfidf.fit_transform(X)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "tuKe4gBj53Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=2000),\n",
        "    \"SVM (LinearSVC)\": LinearSVC(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=300),\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
        "}"
      ],
      "metadata": {
        "id": "xRM86Knw55N8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n=== Training {name} ===\")\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, pred)\n",
        "    prec = precision_score(y_test, pred, average=\"weighted\", zero_division=0)\n",
        "    rec = recall_score(y_test, pred, average=\"weighted\", zero_division=0)\n",
        "    f1 = f1_score(y_test, pred, average=\"weighted\", zero_division=0)\n",
        "\n",
        "    results.append([name, acc, prec, rec, f1])\n",
        "\n",
        "    print(classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "SFhqx43C56vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
        "results_df_sorted = results_df.sort_values(by=\"F1\", ascending=False)\n",
        "\n",
        "print(\"\\n\\n=== HASIL URUT PERFORMA TERBAIK (BERDASARKAN F1) ===\")\n",
        "print(results_df_sorted)"
      ],
      "metadata": {
        "id": "7Kdtv3EQ59lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = results_df_sorted.iloc[0][\"Model\"]\n",
        "best_f1 = results_df_sorted.iloc[0][\"F1\"]\n",
        "\n",
        "print(\"\\n\\n=== MODEL TERBAIK ===\")\n",
        "print(f\"Model terbaik: {best_model_name} (F1-score: {best_f1:.4f})\")"
      ],
      "metadata": {
        "id": "GOQnC_-J6CHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"aryaduta662\"\n",
        "!git config --global user.email \"aryaduta662@gmail.com\""
      ],
      "metadata": {
        "id": "tpnRWpl78vIb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://ghp_816438@github.com/aryaduta662/final_project_streamlit.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OyrBY0196qh",
        "outputId": "e4359e0f-3a17-4944-eafa-c5fad5f764c0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'final_project_streamlit'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 5 (delta 1), reused 5 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (5/5), 27.66 KiB | 1.20 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://ghp_816438@github.com/aryaduta662/final_project_streamlit.git\n"
      ],
      "metadata": {
        "id": "KkMHIDKaA5sB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote -v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWAMwa11Bp-l",
        "outputId": "7f050fb2-124c-43f4-ef41-a1d33ea6eb92"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "origin\thttps://ghp_816438@github.com/aryaduta662/final_project_streamlit.git (fetch)\n",
            "origin\thttps://ghp_816438@github.com/aryaduta662/final_project_streamlit.git (push)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd final_project_streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH3lWNn4-Ou8",
        "outputId": "31a736ac-9c14-4a5a-abd5-526eba30c50a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/final_project_streamlit/final_project_streamlit/final_project_streamlit/final_project_streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zebKVUm_kDz",
        "outputId": "20d3d930-dc14-44f8-8884-deefbdba0f39"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  final_project_streamlit\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/FPDatmin.ipynb\" ."
      ],
      "metadata": {
        "id": "93V14yZX_TAm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhrb4TQfAmZB",
        "outputId": "aa48bc48-f61e-45fb-d3d1-bd7b5887a7be"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py  Dataset_after_preproses_fix.csv  datasetlabel.csv  FPDatmin.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /content/drive/MyDrive -name \"FPDatmin.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnc3mCQGANpn",
        "outputId": "06b5151c-8fee-40f9-b60c-c426dba3b2bf"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/FPDatmin.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySA_jpZmBS9_",
        "outputId": "0ff5223d-c75c-4188-b0e5-2a39f9ffdd8d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Password for 'https://ghp_816438@github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status\n",
        "!git add FPDatmin.ipynb\n",
        "!git commit -m \"add notebook FPDatmin\"\n",
        "!git push origin main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9EY1LQW_1hw",
        "outputId": "d1254dfe-dd3b-4009-d1fe-6aae43da41c8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mFPDatmin.ipynb\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n",
            "[main 2cfda60] add notebook FPDatmin\n",
            " 1 file changed, 1 insertion(+)\n",
            " create mode 100644 FPDatmin.ipynb\n",
            "fatal: could not read Password for 'https://ghp_816438@github.com': No such device or address\n"
          ]
        }
      ]
    }
  ]
}